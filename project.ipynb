{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguishing muffins and Chihuahuas with NNs\n",
    "\n",
    "The aim of this project is to train a neural network for the binary classification of muffins and Chihuahuas based on the images contained in the relative [Kaggle dataset](https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pip\n",
    "\n",
    "def import_or_install(package_name, alias=None):\n",
    "    try:\n",
    "        imported_package = importlib.import_module(package_name)\n",
    "        if alias:\n",
    "            globals()[alias] = imported_package\n",
    "        print(f\"{package_name} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{package_name} is not installed. Installing...\")\n",
    "        pip.install(package_name)\n",
    "        if alias:\n",
    "            globals()[alias] = importlib.import_module(package_name)\n",
    "        print(f\"{package_name} has been installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset from Kaggle, using a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"luciaannamellini\"\n",
    "os.environ['KAGGLE_KEY'] = \"c209fcf223ecdd6be8fd373196354f4b\"\n",
    "\n",
    "import_or_install(\"kaggle\")\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "FOLDERNAME = \"muffin-vs-chihuahua-image-classification\"\n",
    "PATH = \"./\"\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.dataset_download_files(\"samuelcortinhas/muffin-vs-chihuahua-image-classification\", PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(PATH + FOLDERNAME + \".zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(FOLDERNAME)\n",
    "os.remove(PATH + FOLDERNAME +\".zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If already present we import, or otherwise we install, the `tensorflow`package. Above all functionalities we will be using the `keras` module to construct the desired neural network for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_or_install(\"tensorflow\",\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding we remove badly encoded images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def remove_badly_encoded_images(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.JPG'):\n",
    "            try:\n",
    "                img = Image.open(directory+filename)  # open the image file\n",
    "                img.verify()  # verify that it is, in fact an image\n",
    "                img.close()\n",
    "            except (IOError, SyntaxError):\n",
    "                os.remove(directory+filename)\n",
    "\n",
    "deleted = remove_badly_encoded_images(PATH + FOLDERNAME + \"/\")\n",
    "print(\"Deleted images:\",deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by subdividing the dataset into training and test set. This partition is provided by the Kaggle dataset itself, with $\\frac{1}{5}$ of the images assigned to the test set. \n",
    "\n",
    "We also infer the binary labels from the layout of the dataset. Specifically the labels are assigned as follows:\n",
    "\\begin{equation*}\n",
    "    \\begin{cases}\n",
    "        0  & \\text{chihuahua} \\\\\n",
    "        1  & \\text{muffin}\n",
    "    \\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(foldername):\n",
    "    dataset_path = os.path.join(PATH, FOLDERNAME, foldername)\n",
    "    return tf.keras.utils.image_dataset_from_directory(dataset_path, labels = \"inferred\")\n",
    "\n",
    "print(\"Training set:\")\n",
    "train_ds = create_dataset(\"train\")\n",
    "print(\"Test set\")\n",
    "test_ds = create_dataset(\"test\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look at a small sample of pictures with the relative annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the actual pre-processing of the data we prepare a function that produces realistic variations of the images in the dataset. This data augmentation step is useful to make a small dataset more expressive by introducing random changes, so counteracting overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def data_augmentation(images, data_augmentation_layers, training = True):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images, training = training)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we may randomly flip the picture or slightly tilt it. We show here the potential results of the data augmentation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images, data_augmentation_layers, training=True)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make learning easer we simplify the images by:\n",
    "* taking them in grayscale,\n",
    "* reducing their dimensions by resizing them to the size $(64,64)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_grayscale(image):    \n",
    "    grayscale_image = tf.image.rgb_to_grayscale(image)\n",
    "    return grayscale_image\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    resized_image = tf.image.resize(image, target_size)\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "image_size = (64,64)\n",
    "\n",
    "preprocess = {\n",
    "    'rgb_to_grayscale': lambda x, y: (rgb_to_grayscale(x),y),\n",
    "    'resize_image': lambda x,y: (resize_image(x, image_size),y)\n",
    "}\n",
    "\n",
    "for func_name, func in preprocess.items():\n",
    "    train_ds = train_ds.map(func)\n",
    "    test_ds = test_ds.map(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for the reasons stated before, we substitute each of the images in the training set with it's augmented version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(dataset, data_augmentation_function):\n",
    "    dataset.map(data_augmentation_function)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, data_augmentation_layers, training=True), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "input_shape = train_ds.element_spec[0].shape[1:]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds, epochs=1, batch_size=2, validation_data=test_ds)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

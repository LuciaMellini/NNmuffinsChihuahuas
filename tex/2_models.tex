\section{Chosen models}\label{2_models}
In this section we illustrate the hypermodels that have been chosen of our classification problem (Section~\ref{hypermodels}), simplified through a visual representation of their structure. Subsequently, in Section~\ref{hyperparameter_tuning}, we describe how we have tuned the hyperparameters for each of the hypermodels.

\subsection{Hypermodels}\label{hypermodels}
For each of the neural networks we present a visual representation of the structure of the model and a description of the role of each of it's components, to understand the rational behind our choices. We call them hypermodels since in this phase we a only talking about the composition of the models, while the hyperparameters are tuned in a later stage. The \textsl{Hypermodel 1}, described in Section~\ref{hypermodel_1}, is the starting point for various adjustments added in the other hypermodels.

\subsubsection{Hypermodel 1}\label{hypermodel_1}
The first, simpler model, is composed by two convolutional layers, ideal for detecting patterns in images. Also, each of the convolutional layers is followed by a max pooling layer, with the aim of summarizing the information extracted by passing multiple kernels over the images.

After the first learning stage follows a flattening layer, that reduces the expressivity of the foundings, and also simplifies the task left for the next step. Finally, the last output layer is designed to summarize this information into a unique value in the interval $\left[0,1\right]$.

\begin{figure}[h]
    \label{fig:Hypermodel_1}
    \centering
    \includegraphics[scale=0.16]{images/hypermodel_1.png}
    \caption{Structure of Hypermodel 1}
\end{figure}

\subsubsection{Hypermodel 2}\label{hypermodel_2}
We have experimented by adding a dense layer as the last step in the network, to see if a reinterpretation of the information extracted by the convolutional layers would help detect the classes of the images better.

\begin{figure}[h]
    \label{fig:Hypermodel_2}
    \centering
    \includegraphics[scale=0.17]{images/hypermodel_2.png}
    \caption{Structure of Hypermodel 2}
\end{figure}

\subsubsection{Hypermodel 3}\label{hypermodel_3}
For this model we wanted to study the quality of the prediction obtained by adding a convolutional layer followed by a max pooling layer. The idea is that this network should be able to detect more complex patterns in the images.

To assure that all possible combinations of the hyperparameter values would be valid, we have chosen to pad the images during all convolutional stages. Seen that the stride is set to $(1,1)$, the dimensions of the images as input and output of a convolutional layer remain unchanged.

\begin{figure}[!]
    \label{fig:Hypermodel_3}
    \centering
    \includegraphics[scale=0.17]{images/hypermodel_3.png}
    \caption{Structure of Hypermodel 3}
\end{figure}

\subsubsection{Hypermodel 4}\label{hypermodel_4}
Lastly, we were curious to see both the additions to \textsl{Hypermodel 1} applied together.
Also in this case we have chosen to pad the images for the same reason as described in Section~\ref{hypermodel_3}.

\begin{figure}[!]
    \label{fig:Hypermodel_4}
    \centering
    \includegraphics[scale=0.17]{images/hypermodel_4.png}
    \caption{Structure of Hypermodel 4}
\end{figure}

\subsection{Hyperparameter tuning}\label{hyperparameter_tuning}
After having outlined the structure of the neural networks we have chosen the hyperparameters for each layer. Specifically we were looking to choose the number of filters in the convolutional layers, the size of the kernels, the size of the pool in the max pooling layers, and the number of nodes in the dense layers. The possible hyperparameters we have chosen to tune are listed in Table~\ref{tab:hyperparameters}. For all the convolutional layers we have chosen to use the ReLU activation function, while for the output layer we have used the sigmoid function. 

\input{tex/tables/hyperparameterValues}
   
To do this we use the \textit{keras\_tuner} library \cite{omalley2019kerastuner}, to carry out hyperparameter tuning. It allows to build a \textit{Tuner} object based on a compiled Keras model that has as argument a \textit{Hyperparameter} object. Such a tuner can then search for the best possible hyperparameters for the model and value domains that have been supplied. 

For efficiency reasons we have opted for a random search in the hyperparameter space, in fact we have carried out only $200$ trials, so only a portion of the combinations of hyperparameters has been be evaluated. Also, with have limited the number of training epochs to 10. Although the values tuned such way will not necessarily be the best, we were merely interested in having a good approximation of some good hyperparameters.

During the tuning process we have adopted two different approaches, listed below. 
\begin{description}
    \item[Without data augmentation]  In the first case we have trained the models with a given set of hyperparameters using the original training set.
    \item[With data augmentation]  Secondly, to avoid having hyperparameters too dependent on the dataset we have used an augmented version of the training set. Specifically, we have applied a random alteration to each of the images in each epoch of training according to the possible transformations described in Table~\ref{tab:dataAugmentation}.
\end{description}

For each of the two methods we have trained the models with a certain set of hyperparameters with respect to the binary cross-entropy loss. We have then chosen the best hyperparameters measured against the accuracy of the validation set, that has been observed to be at least $0.85$. 

We display the selected hyperparameters for each of the hypermodels in Tables~\ref{tab:chosenHyperparameters}\subref{tab:chosenHyperparameters_tuners} and~\ref{tab:chosenHyperparameters}\subref{tab:chosenHyperparameters_tuners_augmented}. From here on we call the models obtained by setting the hyperparameters according to the best values extracted by training on the original dataset and the augmented one, respectively, \textsl{Model 1} to \textsl{Model 4} and \textsl{Model\_aug 3} to \textsl{Model\_aug 4}.

\input{tex/tables/chosenHyperparameters}







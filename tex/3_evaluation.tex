\section{Evaluation of the models}\label{3_evaluation}
In this section we evaluate the various neural networks according to their risk with respect to zero-one loss. We will estimate such measure using $5$-fold cross validation. We have chosen to round the predicted labels in $\left[0,1\right]$ to the nearest digit. Also, to reduce computational time we have decided to stop the training for a fold when the loss had not decreased for 10 subsequent epochs. 

To have a broader view on the quality of the models we have carried out cross validation on the original dataset, formed by the training, validation and test sets, and a dataset composed only by the training and test sets. In fact, seen that the validation set has been used to score the best hyperparameters for each of the models, one expects that the risk of the predictor would be lower seen that it has been calibrated on the validation set. 

We evaluate against both risk estimation versions for the models obtained by setting the hyperparameters according to the best values extracted by training on the original dataset and the augmented one (see Section~\ref{hyperparameter_tuning}). All the results are gathered in Table~\ref{tab:riskEstimates}.

\input{tex/tables/riskEstimates}

\subsection{Original dataset}\label{evaluation_originalDataset}
In this section we look at the risk estimations found by carrying out $5$-fold cross validation on the original dataset, composed by the training, test and validation sets. 

We note that generally the risk for the models for which the hyperparameters have been chosen based on the augmented dataset is lower than the ones obtained by training on the original dataset. This is expected, since the models have been exposed to different versions of the images in the dataset, so they are more equipped to classify new images.

\paragraph{Hyperparameters chosen based on original dataset}
In the case of the models with hyperparameters chosen by looking at the original training set the difference in quality between the various models is more evident. In particular we see that \textsl{Model 1} and \textsl{Model 3} perform slightly better. Looking back at the structure of the hypermodels (see Section~\ref{hypermodels}), this indicates that the addition of a dense layer at the end of the network does not help in the classification task. We can explain this by observing that the knowledge about the images is extracted at the convolutional level, so an additional reworking of these features only adds noise to the prediction.

\paragraph{Hyperparameters chosen based on augmented dataset}
Vice versa, seen the results for the models obtained by choosing the hyperparameters based on an augmented version of the dataset, there is no clear winner. This is possibly due to the fact that the models have been exposed to a more varied dataset, so the quality of the prediction is more balanced. We note a slight advantage of \textsl{Model\_aug 2} and \textsl{Model\_aug 3}, implying that the simplest and the more complex solutions are not ideal.

\subsection{Dataset without validation set}\label{evaluation_datasetWithoutVal}
In this section we look at the risk estimations found by carrying out $5$-fold cross validation on the images in the training and test set.

In this case the results for \textit{Model 1} to \textit{Model 4} are similar to \textit{Model\_aug 1} to \textit{Model\_aug 4}. This is due to the fact that the risk estimates have been computed on images that have not been used to choose the best hyperparameters, so the results are less influenced by the alterations on the dataset. It is also to be said that the transformations applied to the pictures were not drastic.

We also observe that with respect to the results seen for cross validation carried out on the original dataset, analyzed in Section~\ref{evaluation_originalDataset}, there is a major discrepancy between the quality of different models. This indicates that by excluding the validation set we have a better indication of the quality of the models.

As suggested by the results extracted from cross validation on the original dataset on the models obtained through training on the unaltered images in the training set, the risk estimates for \textsl{Model 2}, \textsl{Model\_aug 2}, \textsl{Model 4} and \textsl{Model\_aug 4} are higher than the ones for \textsl{Model 1}, \textsl{Model\_aug 1}, \textsl{Model 3} and \textsl{Model\_aug 3}. This indicates, even more strongly, that the additional dense layer does not have added value to the prediction. Also, the risk estimates for \textsl{Model 3} and \textsl{Model\_aug 3} are higher than the ones for \textsl{Model 1} and \textsl{Model\_aug 1}, suggesting that the additional convolutional layer followed by a max pooling layer does not better the quality of the prediction. Possibly, a less complex network is better at grasping the patterns in the images, as an instance of Occam's Razor.
\section{Data pre-processing}\label{1_preProcessing}
In this section we list the pre-processing pipeline that has been applied to the training and test set already supplied by Kaggle.

\paragraph{Remove badly encoded images}
We remove all the images that are badly encoded. We do this by simply trying to open the image and verifying that the file is not broken. At this scope we have used the \texttt{verify} method of the \texttt{Image} object made available by the \texttt{PIL}(Python Imaging Library) package. Citing the documentation this method \textit{attempts to determine if the file is broken, without actually decoding the image data. If this method finds any problems, it raises suitable exceptions}.

\paragraph{Data augmentation}
To make the dataset more expressive and avoid overfitting, we have applied random changes to the images. It would also be possible to enlarge the size of the dataset by introducing new images created through these transformations, but we have chosen to not add images for efficiency reasons during the training and evaluation stages. We randomly flip the pictures according to their vertical axes, and/or we randomly rotate them of an angle in the range $\left[-20\% \cdot 2\pi, 20\% \cdot 2\pi\right]$.

\paragraph{Simplifying the image samples}
To make training easier and more efficient we have considered the images in grayscale instead of RGB, so reducing the number of channels that describe a sample from 3 to 1. For the same reason we have reduced the dimensions of all pictures by rescaling them to the size $\left(64,64\right)$. This size has been chosen to balance the need of having a good resolution to recognize the digits and the need of having a small size to make the training process faster. We have chosen to apply these modifications to the training set and the test set instead of inserting this step at the beginning of the training process to avoid a computational overhead during model execution.

\subsection{Partitioning}
We have assumed that the order of the samples in the dataset was arbitrary, so we have relied on the subdivision provided by Kaggle. The chosen Kaggle dataset is subdivided into training and test set, with $\frac{1}{5}$ of the images assigned to the test set. We have chosen to further partition the training set into an actual training set and a validation set to tune the hyperparameters for the proposed hypermodels. So, we have partitioned the images in three groups as follows:
\begin{description}
    \item[training set] $70\%$ of the samples,
    \item[validation set] $10\%$ of the samples,  
    \item[test set] $20\%$ of the samples. 
\end{description}